{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3jJ5370vrocXmNEP8Rbxv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIOLDAVE/my_project/blob/main/Massive_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9dY0sWM2uhlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project requirement: https://docs.google.com/document/d/1GkK28wOyjTPZEZuOumKPU0z1NzVT_9sxETjPOcLPVYo/edit?tab=t.0#heading=h.qifoo7co6qtd\n",
        "\n",
        "Professor : https://malchiodi.di.unimi.it/teaching/AMD-DSE/2024-25/en"
      ],
      "metadata": {
        "id": "1jlTqLeCltAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting the notebook to my_project directory in GitHub."
      ],
      "metadata": {
        "id": "wOdfAs20n8dX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4JVmRCglg7N",
        "outputId": "85ff0815-bfe0-4c52-8c27-1c09c3f99be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'my_project'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 50 (delta 27), reused 17 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (50/50), 37.63 KiB | 3.42 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"my_project\"):\n",
        "    !git clone https://github.com/VIOLDAVE/my_project.git\n",
        "\n",
        "if os.path.exists(\"my_project\"):\n",
        "    os.chdir(\"my_project\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Failed to find or clone the 'my_project' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages."
      ],
      "metadata": {
        "id": "RHFvE1XowXFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "32iMmIFL04_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f49c13-9d50-4ad2-8671-9b71d430f6a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 1)) (1.7.4.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 4)) (3.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirement.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle->-r requirement.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark->-r requirement.txt (line 4)) (0.10.9.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirement.txt (line 5)) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirement.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirement.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirement.txt (line 6)) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " kaggle  set up and Authentication"
      ],
      "metadata": {
        "id": "1ZAJt1DeWM6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "\n",
        "\n",
        "    # Move and rename the uploaded file to the correct kaggle.json path\n",
        "    os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "    os.rename(fn, \"/root/.kaggle/kaggle.json\")\n",
        "    os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Cwgtv5FH0AV-",
        "outputId": "367ab5cf-f5c5-4fd5-f09a-a4b9c6b5e7d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2930e51-6b4d-411c-8b03-abc86449ac36\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2930e51-6b4d-411c-8b03-abc86449ac36\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle2.json to kaggle2.json\n",
            "User uploaded file \"kaggle2.json\" with length 66 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dowloading files"
      ],
      "metadata": {
        "id": "fWmVG7a4rLko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZQyNBL2qyZ1",
        "outputId": "cca4a5db-2860-49af-bf12-1dac5f1c977c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content/my_project\n",
            " 99% 1.05G/1.06G [00:12<00:00, 259MB/s]\n",
            "100% 1.06G/1.06G [00:14<00:00, 78.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzipping the dataset"
      ],
      "metadata": {
        "id": "GUSv7ijrL074"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"/content/my_project/amazon-books-reviews.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")\n",
        "\n",
        "# List the extracted files to confirm\n",
        "!ls /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pV31iADR5Tb",
        "outputId": "6f7001c6-5d53-4960-e608-86c6a498e9d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/data\n",
            "books_data.csv\tBooks_rating.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset on Spark"
      ],
      "metadata": {
        "id": "aevDm3w5rY9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset with spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "books_rating = spark.read.csv(\"/content/data/Books_rating.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "hBLdTQKIrZWf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_rating.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkZURsNLlAyR",
        "outputId": "8afd5601-50a0-4808-e8d9-de5fcc10d671"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books_rating.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_tSdTjviQj2",
        "outputId": "b14939ca-36e2-4150-858e-6f475394f7e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[summary: string, Id: string, Title: string, Price: string, User_id: string, profileName: string, review/helpfulness: string, review/score: string, review/time: string, review/summary: string, review/text: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show a few raws of the dataset"
      ],
      "metadata": {
        "id": "GZ5HFPgh3u_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_rating.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0eAgajY3yXX",
        "outputId": "a9efdf58-1fb5-4982-b694-d1d6d0edc80d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|1882931173|Its Only Art If I...| NULL| AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|\"If people become...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A2MVUWT453QH61|\"Roy E. Perry \"\"a...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A22X4XUPKF66MR|\"D. H. Richards \"...|               3/3|         4.0| 1107993600|Good academic ove...|\"Philip Nel - Dr....|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a sample of dataset becase the file is too heavy for my colab space"
      ],
      "metadata": {
        "id": "_itQjlr433Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 0.05 # can be adjusted as needed"
      ],
      "metadata": {
        "id": "_G19RGI633rI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "books_sample.show(10)\n",
        "size = books_sample.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_94ae_l48G9",
        "outputId": "98cc2f32-1bfe-4ceb-a3a8-c4ac2727968f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|\n",
            "|B000879GGE|A husband for Kutani| NULL|A2HDZHLMT3L5IO|\"Laurence Bush \"\"...|               0/0|         5.0| 1141603200|Unique Weird Orie...|\"Exotic tales of ...|\n",
            "|0553763121|Overcoming Hypert...|23.00|A3TQFPF0QGVSJV|   \"Woody \"\"Woody\"\"\"|               2/2|         5.0| 1352073600|A must read for a...|\"Books by Dr Kenn...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A2GA412HQHN8WV|  Elizabeth Szymczak|               0/0|         5.0| 1275696000|great book---but....|\"This book is gre...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A1CEJFXSJYQBTX|\"J. Hauser \"\"Yell...|               3/3|         5.0| 1157241600|The Travelers Com...|\"I admit it: I'm ...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|\n",
            "|006000486X|Tess and the High...| NULL|A148K8LOPWE6RK|                Jess|               0/0|         5.0| 1306281600|               &lt;3|I'm not a teenage...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQhYzI7B5b1F",
        "outputId": "f2c301c9-f8cc-4002-9a05-e871db765186"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150324"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking null values. I will use the review column which has strings."
      ],
      "metadata": {
        "id": "e1JQrTWM560x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking  for null values in the 'review/text' column\n",
        "null_df = books_rating.filter(\n",
        "    books_rating['review/text'].isNull() | (books_rating['review/text'].rlike(r'^\\s*$'))\n",
        ")\n",
        "\n",
        "null_df.show(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJLfprIv6U-c",
        "outputId": "b1d0fcbc-ce58-4894-db8c-beec829260c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|        Id|               Title|               Price|             User_id|         profileName|  review/helpfulness|        review/score|         review/time|      review/summary|review/text|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|B00085T7O2|\"\"\"Catch 'em aliv...|\",,ASEH0CVYVGZCZ,...|                 0/1|                 5.0|          1341705600|Fascinating life ...|There are many id...|                NULL|       NULL|\n",
            "|B00085T7O2|\"\"\"Catch 'em aliv...|\",,A91QQ7IYBAK2Q,...| terrible forward...|I found the book ...|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0007DMFZS|\"\"\"Beloved friend...|\",,AH9F75LQIKEZH,...|                 0/0|                 5.0|          1268611200|Invaluable path t...|\"There is so much...| An American Busi...|       NULL|\n",
            "|B00085P4PI|\"The child Manuel...|\",,A3HN2H8SWQ9V0Z...|                 1/1|                 5.0|          1106438400|Amazing and touching|This book was sim...|                NULL|       NULL|\n",
            "|B0007EFQHG|\"The Nazi \"\"88\"\" ...|\",,,,1/1,5.0,1127...| easy to read and...| Happy P. Abbott....| of any age will ...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0007EFQHG|\"The Nazi \"\"88\"\" ...|\",,A24FY5HONGOBA5...|\"\" it fortified m...|                NULL|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0008CK87U|\"\"\"Gentlemen pref...|\",,,,3/3,5.0,9070...| a directness tha...|       not a cartoon| and therefore ev...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B000GQK706|The Lord of the R...|                NULL|                NULL|                NULL|                 0/1|                 5.0|           938563200|have only one wor...|       NULL|\n",
            "|B00085MA6E|\"Sins of New York...|\",,AT3NJHZMT67KH,...|   Mediocre Stories\"|I must say that I...|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|096719587X|\"Anabolic Outlaw ...|         pot smoking|    cocaine snorting|        pill popping|       acid dropping|    whiskey drinking| steroid shooting...|                4.95|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|\",,A1IZWTVAQVJIFE...| with surprising ...|Looney's double-o...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|\",,A2SBK4UJEMLDPW...| the main thrust ...| the assertion th...| this will change...| now make sense i...|                NULL|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|,,A3Q5N4H6DYX2KP,...|                NULL|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B000I1VJLA|The Lord of the R...|                NULL|                NULL|                NULL|                 0/1|                 5.0|           938563200|have only one wor...|       NULL|\n",
            "|1594860882|Scrawny to Brawny...|               13.18|      A2PZ208GDOIWZM|\"sloth \"\"\\|/ 3Toe...| and are quoted o...| the best site ou...|                NULL|                NULL|       NULL|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are copies of the'review/text' null entries.\n",
        "\n",
        " Aside from the Title and the IDs, they have the same columns.  A qualitative examination reveals that they are about the same book even though the titles are different."
      ],
      "metadata": {
        "id": "fof3zZqN7PCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are numerous other reviews of Lord of the Rings.  I believe that the NA values should be retained because they cannot be discarded.  The title can also be used as a similarity check."
      ],
      "metadata": {
        "id": "6qeX0MHDAbu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filter_df = books_rating.filter(\n",
        "    col(\"Title\").contains(\"Wonderful Worship in Smaller Churches\")\n",
        ").select(\"Title\", \"review/text\").limit(100)\n",
        "\n",
        "# result\n",
        "filter_df.show(100, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okOXjXCbAfvQ",
        "outputId": "84a96c30-b8ae-4f4a-c48a-b754c3ffbe47"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               Title|         review/text|\n",
            "+--------------------+--------------------+\n",
            "|Wonderful Worship...|I just finished t...|\n",
            "|Wonderful Worship...|\"Many small churc...|\n",
            "|Wonderful Worship...|I just finished r...|\n",
            "|Wonderful Worship...|\"I hadn't been a ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null values in subsample books."
      ],
      "metadata": {
        "id": "VQqnaZkRBJIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the 'review/text' column\n",
        "Subsample_book = books_sample.filter(\n",
        "    books_sample['review/text'].isNull() | (books_sample['review/text'].rlike(r'^\\s*$'))\n",
        ")\n",
        "#Display sample books\n",
        "Subsample_book.show(35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW6xSRe2Beju",
        "outputId": "3b5561a9-5a7c-4a85-987d-b148d6213096"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|        Id|               Title|               Price|             User_id|         profileName|review/helpfulness|        review/score|         review/time|      review/summary|review/text|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|B000I1VJLA|The Lord of the R...|                NULL|                NULL|                NULL|               0/1|                 5.0|           938563200|have only one wor...|       NULL|\n",
            "|B00085H0VE|\"\"\"Charge that to...|\"\" and other Gosp...|\",,A2AKETSGUW9KYI...|                 1/1|               5.0|          1211760000|One of the best p...|This little book ...|       NULL|\n",
            "|B0006AQJN6|\"\"\"A giant in the...|\"\": A biography o...|\",,AADCWGMKN9QNE,...| historical and f...|              NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B00085P8PO|\"Around the world...|\",,A24K5Y2GCIKHRV...|                 0/0|                 5.0|        1336435200|A fine example of...|Harry Pidgeon is ...|                NULL|       NULL|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Similarity\n",
        "\n",
        "\n",
        "#Tokenisation\n",
        " To test the Jaccard Similarity technique, I will utilise a subsample of the dataset because it is too large.  After that, I'll attempt to top scale it to the entire dataset.  Additionally, I'll parallelise the Jaccard Similarity calculation using the MapReduce technique."
      ],
      "metadata": {
        "id": "18-3_LwWYpla"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ea97f3"
      },
      "source": [
        "## Apply tokenization\n",
        "\n",
        "### Subtask:\n",
        "Apply the `RegexTokenizer` (to break down the text into individual words or tokens.) and create new column the `review/text`  of  Spark DataFrame to get tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ec07469"
      },
      "source": [
        "**Meaning**\n",
        "It applies the pre-defined RegexTokenizer to the books_sample DataFrame and displays the schema of the resulting DataFrame to confirm the new column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDV9F9W9e4ls"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxIwTDSUYB1E",
        "outputId": "79a235b9-f355-4ae7-cd4e-3ff805f9cbe2"
      },
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=r\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8894ba39"
      },
      "source": [
        "## Remove stop words\n",
        "\n",
        "### Subtask:\n",
        "Use `StopWordsRemover` to remove common English stop words from the tokenized reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889f81da"
      },
      "source": [
        "**Reasoning**:\n",
        "Use StopWordsRemover to remove common English stop words from the tokenized reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eaf9030",
        "outputId": "eec7c880-ab15-40ab-db6e-6d7742cdff2d"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"reviews/filtered_tokens\")\n",
        "filtered_df = remover.transform(tokenized_df)\n",
        "filtered_df.printSchema()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- reviews/filtered_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f0e3b8"
      },
      "source": [
        "## Calculate jaccard similarity\n",
        "\n",
        "### Subtask:\n",
        "Implementing a method to calculate the Jaccard similarity between the processed review tokens for pairs of books.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc5dcc80"
      },
      "source": [
        "\n",
        " Python function to calculate Jaccard similarity and register it as a Spark UDF. Then, join the DataFrame with itself to create pairs of books and apply the UDF to calculate the similarity between their filtered tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f5af91",
        "outputId": "20813dc9-3b76-48c0-c611-1650ea5dbbef"
      },
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    if not list1 or not list2:\n",
        "        return 0.0\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0.0\n",
        "\n",
        "jaccard_udf = udf(jaccard_similarity, DoubleType())\n",
        "\n",
        "# Create pairs of books for similarity calculation\n",
        "book_pairs = filtered_df.alias(\"df1\").join(\n",
        "    filtered_df.alias(\"df2\"),\n",
        "    col(\"df1.Id\") < col(\"df2.Id\")\n",
        ")\n",
        "\n",
        "# Calculate Jaccard similarity for each pair\n",
        "similarity_df = book_pairs.withColumn(\n",
        "    \"jaccard_similarity\",\n",
        "    jaccard_udf(col(\"df1.reviews/filtered_tokens\"), col(\"df2.reviews/filtered_tokens\"))\n",
        ")\n",
        "\n",
        "# Select relevant columns and show the results\n",
        "similarity_df.select(\n",
        "    col(\"df1.Id\").alias(\"book1_id\"),\n",
        "    col(\"df2.Id\").alias(\"book2_id\"),\n",
        "    col(\"jaccard_similarity\")\n",
        ").show(20)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+\n",
            "|  book1_id|  book2_id|  jaccard_similarity|\n",
            "+----------+----------+--------------------+\n",
            "|0595344550|B000879GGE|                 0.0|\n",
            "|0595344550|B000NKGYMK|0.056338028169014086|\n",
            "|0595344550|B000NKGYMK|0.013513513513513514|\n",
            "|0595344550|0789480662| 0.07407407407407407|\n",
            "|0595344550|0789480662|0.012048192771084338|\n",
            "|0595344550|157067051X|0.015151515151515152|\n",
            "|0595344550|B0007DVHU2|0.009523809523809525|\n",
            "|0595344550|0781810698|0.013888888888888888|\n",
            "|0595344550|B0006DWYDW|0.015384615384615385|\n",
            "|0595344550|3526448353|0.034482758620689655|\n",
            "|0595344550|B0006D6DRK|  0.0425531914893617|\n",
            "|0595344550|0934638160|0.013333333333333334|\n",
            "|0595344550|0934638160|0.012987012987012988|\n",
            "|0595344550|B000FZEKVA|0.018691588785046728|\n",
            "|0595344550|1861263678|0.030303030303030304|\n",
            "|0595344550|0671551345|0.029850746268656716|\n",
            "|0595344550|0671551345|0.011363636363636364|\n",
            "|0595344550|0671551345|0.029850746268656716|\n",
            "|0595344550|B000N7612G| 0.02702702702702703|\n",
            "|0595344550|B000N7612G|                 0.0|\n",
            "+----------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Jaccard similarity"
      ],
      "metadata": {
        "id": "nKGtYB6PirsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType"
      ],
      "metadata": {
        "id": "UHZbsToRjhyS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=r\"\\W\") # \\W is a regex pattern that matches any non-word character"
      ],
      "metadata": {
        "id": "-GMXb3xqnf0Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5518f3b0"
      },
      "source": [
        "The text in the reviews is tokenized into individual words using Spark's `RegexTokenizer`. This process is a foundational step for many text analysis tasks. Using Spark for tokenization leverages its distributed processing capabilities, which can be faster for large datasets compared to certain methods, including some neural network-based approaches that might require different preprocessing or computational resources.\n",
        "\n",
        "While effective for breaking down text, this tokenization method has limitations, such as not explicitly handling compound terms (like \"well-being\") as single units. However, for the initial strategy of calculating Jaccard similarity based on individual word sets, this approach is a suitable starting point."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"tokens/nostopwords\")"
      ],
      "metadata": {
        "id": "FxqAfBW7nvDc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3857585d"
      },
      "source": [
        "The `StopWordsRemover` uses a predefined list of common English stop words (like \"the,\" \"is,\" \"in\") and removes them from the tokenized reviews. This step is crucial because these common words appear frequently in most texts but carry little specific meaning. For methods like Jaccard Similarity, which rely on the overlap of unique terms between documents, including stop words would inflate the similarity scores based on irrelevant terms, skewing the results. Removing them helps to focus on the more meaningful words that truly contribute to the content's uniqueness and similarity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "    return [token for token in tokens if not token.isdigit()]"
      ],
      "metadata": {
        "id": "87T7-u0wo-61"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequently, the meaning of a review is not affected by numbers.  We can eliminate the numbers from the text because the score in our dataset is kept in a distinct column."
      ],
      "metadata": {
        "id": "P8y7jQDcpEly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_numbers_udf = udf(remove_numbers, ArrayType(StringType())) # Specify the return type as ArrayType(StringType), meaning an array/list of strings\n"
      ],
      "metadata": {
        "id": "ANn2O-vCpck_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because Sparks is not a Python object, it must be used in the pipeline as a user-defined function (UDF)."
      ],
      "metadata": {
        "id": "IJZz_8VcpfZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_books_sample = tokenizer.transform(books_sample) # Apply the tokenizer to the dataset\n",
        "tokenized_reviews_nostopwords_sample = stopwords_remover.transform(tokenized_books_sample) # Remove the stop words from the tokenized dataset\n",
        "tokenized_rw_nsw_nn_sample = tokenized_reviews_nostopwords_sample.withColumn(\"tokens/nostopwords/nonumbers\", remove_numbers_udf(col(\"tokens/nostopwords\"))) # Remove the numbers from the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "vLzVFbMapo8H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uvkyH5H6tVHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "books_sample.show(10)\n",
        "tokenized_rw_nsw_nn_sample.show(10, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLD-3q9tZvO",
        "outputId": "41419a3c-af11-4a9f-c737-790bcdc7dc5b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|\n",
            "|B000879GGE|A husband for Kutani| NULL|A2HDZHLMT3L5IO|\"Laurence Bush \"\"...|               0/0|         5.0| 1141603200|Unique Weird Orie...|\"Exotic tales of ...|\n",
            "|0553763121|Overcoming Hypert...|23.00|A3TQFPF0QGVSJV|   \"Woody \"\"Woody\"\"\"|               2/2|         5.0| 1352073600|A must read for a...|\"Books by Dr Kenn...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A2GA412HQHN8WV|  Elizabeth Szymczak|               0/0|         5.0| 1275696000|great book---but....|\"This book is gre...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A1CEJFXSJYQBTX|\"J. Hauser \"\"Yell...|               3/3|         5.0| 1157241600|The Travelers Com...|\"I admit it: I'm ...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|\n",
            "|006000486X|Tess and the High...| NULL|A148K8LOPWE6RK|                Jess|               0/0|         5.0| 1306281600|               &lt;3|I'm not a teenage...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|[this, is, a, sel...|[self, published,...|        [self, published,...|\n",
            "|B000879GGE|A husband for Kutani| NULL|A2HDZHLMT3L5IO|\"Laurence Bush \"\"...|               0/0|         5.0| 1141603200|Unique Weird Orie...|\"Exotic tales of ...|[exotic, tales, o...|[exotic, tales, o...|        [exotic, tales, o...|\n",
            "|0553763121|Overcoming Hypert...|23.00|A3TQFPF0QGVSJV|   \"Woody \"\"Woody\"\"\"|               2/2|         5.0| 1352073600|A must read for a...|\"Books by Dr Kenn...|[books, by, dr, k...|[books, dr, kenne...|        [books, dr, kenne...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|[after, having, a...|[chance, read, bo...|        [chance, read, bo...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|[this, book, is, ...|[book, worth, kee...|        [book, worth, kee...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A2GA412HQHN8WV|  Elizabeth Szymczak|               0/0|         5.0| 1275696000|great book---but....|\"This book is gre...|[this, book, is, ...|[book, great, dk,...|        [book, great, dk,...|\n",
            "|0789480662|Eyewitness Travel...| NULL|A1CEJFXSJYQBTX|\"J. Hauser \"\"Yell...|               3/3|         5.0| 1157241600|The Travelers Com...|\"I admit it: I'm ...|[i, admit, it, i,...|[admit, m, addict...|        [admit, m, addict...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|[gave, me, a, det...|[gave, detailed, ...|        [gave, detailed, ...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|[dr, baker, was, ...|[dr, baker, one, ...|        [dr, baker, one, ...|\n",
            "|006000486X|Tess and the High...| NULL|A148K8LOPWE6RK|                Jess|               0/0|         5.0| 1306281600|               &lt;3|I'm not a teenage...|[i, m, not, a, te...|[m, teenager, app...|        [m, teenager, app...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "688cbf5f"
      },
      "source": [
        "To prepare the tokens for further analysis and to create a representation of the text data, I used `CountVectorizer`. This tool builds a vocabulary of the most frequent tokens in the dataset.\n",
        "\n",
        "*   **`vocabSize`**: This parameter sets the maximum number of unique tokens to include in the vocabulary. By limiting the vocabulary size, we focus on the most relevant terms and manage the dimensionality of the data.\n",
        "*   **`minDF`**: This parameter specifies the minimum number of documents a token must appear in to be considered for the vocabulary. This is important for filtering out very rare tokens that are unlikely to be useful for similarity calculations or other downstream tasks.\n",
        "\n",
        "Using `CountVectorizer` create a structured representation of the text based on the frequency of important words."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "df_safe = tokenized_rw_nsw_nn_sample \\\n",
        "    .filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens/nostopwords/nonumbers\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model = cv.fit(df_safe)\n",
        "vocab = model.vocabulary\n"
      ],
      "metadata": {
        "id": "yIBBW7DItmmf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = model.vocabulary[:30]\n",
        "print(\"Top 30 tokens (approx. by frequency):\")\n",
        "for i, token in enumerate(top_words, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KhrRDwmuyWx",
        "outputId": "097c3b63-e85b-4ca6-9cba-45801ca81be9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 tokens (approx. by frequency):\n",
            " 1. book\n",
            " 2. read\n",
            " 3. one\n",
            " 4. quot\n",
            " 5. story\n",
            " 6. like\n",
            " 7. books\n",
            " 8. good\n",
            " 9. time\n",
            "10. great\n",
            "11. reading\n",
            "12. life\n",
            "13. first\n",
            "14. well\n",
            "15. many\n",
            "16. much\n",
            "17. people\n",
            "18. also\n",
            "19. really\n",
            "20. love\n",
            "21. characters\n",
            "22. novel\n",
            "23. even\n",
            "24. get\n",
            "25. way\n",
            "26. author\n",
            "27. think\n",
            "28. written\n",
            "29. world\n",
            "30. years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top 30 terms in the vocaboulary are roughly represented by this way.  However, since CountVectorizer() merely approximates the actual frequency of the top words, it does not display the frequency, I'm looking for"
      ],
      "metadata": {
        "id": "9p4d5TqUwhdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "exploded = df_safe.select(explode(col(\"tokens/nostopwords/nonumbers\")).alias(\"token\"))\n",
        "\n",
        "# Filtering only the top-20 tokens\n",
        "topK_set = set(model.vocabulary[:20])  # top list  20 from vocabulary\n",
        "filtered = exploded.filter(col(\"token\").isin(topK_set))\n",
        "\n",
        "exact_counts = (filtered.groupBy(\"token\").count().orderBy(col(\"count\").desc()))\n",
        "exact_counts.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNBPHJfywlA7",
        "outputId": "4f1de3e8-03a2-4e15-8089-176b1d352910"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|token  |count |\n",
            "+-------+------+\n",
            "|book   |239998|\n",
            "|read   |99731 |\n",
            "|one    |72419 |\n",
            "|quot   |65743 |\n",
            "|story  |49194 |\n",
            "|like   |45138 |\n",
            "|books  |36776 |\n",
            "|good   |36210 |\n",
            "|time   |36095 |\n",
            "|great  |35893 |\n",
            "|reading|33765 |\n",
            "|life   |32685 |\n",
            "|first  |31778 |\n",
            "|well   |31100 |\n",
            "|many   |30319 |\n",
            "|much   |27813 |\n",
            "|people |26363 |\n",
            "|also   |26269 |\n",
            "|really |26079 |\n",
            "|love   |26017 |\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the exact frequency of the top 20 words from the vocabulary. It shows the actual counts."
      ],
      "metadata": {
        "id": "8WyTscACx7fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.05 * size # % of the sample size and can be adjust\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "OMrH5FLX1m_O",
        "outputId": "39efd6f0-56dd-421a-fd4f-18a5f10de4f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7516.200000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating a frequency threshold for tokens based on a percentage of the sample size (size)."
      ],
      "metadata": {
        "id": "S1PBqaL55_9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest_freq_tokens = (exact_counts.filter(col(\"count\") > threshold).select(\"token\")\n",
        "      .rdd.flatMap(lambda x: x)\n",
        "      .collect()\n",
        ")\n"
      ],
      "metadata": {
        "id": "VS1YHVN819WT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_freq_tokens"
      ],
      "metadata": {
        "id": "8y35tguG2rFZ",
        "outputId": "ffadd50b-7b0e-442f-c39f-8af97a3e0f74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book',\n",
              " 'read',\n",
              " 'one',\n",
              " 'quot',\n",
              " 'story',\n",
              " 'like',\n",
              " 'books',\n",
              " 'good',\n",
              " 'time',\n",
              " 'great',\n",
              " 'reading',\n",
              " 'life',\n",
              " 'first',\n",
              " 'well',\n",
              " 'many',\n",
              " 'much',\n",
              " 'people',\n",
              " 'also',\n",
              " 'really',\n",
              " 'love']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving high-frequency token words."
      ],
      "metadata": {
        "id": "G2pxbbM36Ixq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scalability"
      ],
      "metadata": {
        "id": "XLHhr5bo3Msl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must filter out the entries that contain empty lists because Sparks does not function properly with them.  To accomplish this, I may use the size() and isNull() functions to determine whether the list is null or empty.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "alck-5iA-ZOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering out stop words in the tokenised data."
      ],
      "metadata": {
        "id": "dz1vRb4PiLel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_custom_stopwords(tokens):\n",
        "    return [t for t in tokens if t not in highest_freq_tokens]\n",
        "\n",
        "remove_udf = udf(remove_custom_stopwords, ArrayType(StringType()))\n",
        "\n",
        "tokenized_sample_2 = tokenized_rw_nsw_nn_sample.withColumn(\n",
        "    \"tokens_clean\",\n",
        "    remove_udf(col(\"tokens/nostopwords/nonumbers\"))\n",
        ")"
      ],
      "metadata": {
        "id": "41bSZZPqg49F"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "df_safe_clean = tokenized_sample_2.filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "empty_tokens_count = df_safe_clean.filter(\n",
        "    (size(col(\"tokens_clean\")) == 0) |\n",
        "    (col(\"tokens_clean\").isNull())\n",
        ").count()\n",
        "\n",
        "print(f\"clean empty tokens: {empty_tokens_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAIdHqBViEuu",
        "outputId": "75818192-a4c3-4acd-c878-12ae6cef610a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean empty tokens: 275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering rows with empty list becasue spark doesnot work well with lists."
      ],
      "metadata": {
        "id": "3V20rhSNjKOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens_clean\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model2 = cv.fit(df_safe_clean)\n",
        "vocab2 = model2.vocabulary\n"
      ],
      "metadata": {
        "id": "Vt5S1H9zBMaB"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking for rows where the tokens_clean column contains an empty list or is null."
      ],
      "metadata": {
        "id": "nQIfjh989KKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "different top words"
      ],
      "metadata": {
        "id": "qdw_iioOCtUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top20 = model2.vocabulary[:20]\n",
        "print(top20)\n",
        "for i, token in enumerate(top20, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "id": "g5p35TUmBvP9",
        "outputId": "9d75ae34-aef9-427f-9546-4f67fd892736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['characters', 'novel', 'even', 'get', 'way', 'author', 'think', 'written', 'world', 'years', 'know', 'best', 'new', 'work', 'find', 'little', 'found', 'never', 'make', 'two']\n",
            " 1. characters\n",
            " 2. novel\n",
            " 3. even\n",
            " 4. get\n",
            " 5. way\n",
            " 6. author\n",
            " 7. think\n",
            " 8. written\n",
            " 9. world\n",
            "10. years\n",
            "11. know\n",
            "12. best\n",
            "13. new\n",
            "14. work\n",
            "15. find\n",
            "16. little\n",
            "17. found\n",
            "18. never\n",
            "19. make\n",
            "20. two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the CountVectorizer again, but this time to the tokens_clean column, which contains the tokens after removing stop words, numbers, and your custom high-frequency words.\n",
        "\n",
        "Similar to the previous use of CountVectorizer, this step is building a vocabulary of the most frequent remaining tokens and will convert the text data into numerical feature vectors based on these tokens.\n",
        "\n",
        "The parameters vocabSize=5000 and minDF=10 are used again to limit the vocabulary size to the top 5000 most frequent tokens and only include tokens that appear in at least 10 documents. This process prepares the cleaned text data for the next step, which involves converting these token counts into a format suitable for similarity calculations using HashingTF."
      ],
      "metadata": {
        "id": "vvSpbCzL-HSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Duplicates\n",
        "df_safe = df_safe_clean.dropDuplicates([\"review/text\"])"
      ],
      "metadata": {
        "id": "sFsRNb0gB8XF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Local Sensitive Hashing\n",
        "is a method that hashes similar input items into the same \"buckets\" with high probability. It is used to find similar items in large datasets efficiently."
      ],
      "metadata": {
        "id": "af7c_igIC8Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_threshold = 0.05 # adjust as needed\n"
      ],
      "metadata": {
        "id": "IJN0ZB1XDnI6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_safe.show(10)"
      ],
      "metadata": {
        "id": "INPdEKqgD1ev",
        "outputId": "2ed0a69b-6124-45d9-c00e-36afc94f645d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|        tokens_clean|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "|0595229751|Tranceformers: Sh...| NULL|A3N4LVV9SE7BM8|\"Shea Bower \"\"Dr....|             15/20|         5.0| 1159401600|           \"\"\"Lights|        ..... Action|            [action]|            [action]|                    [action]|            [action]|\n",
            "|0736605010|   Wuthering Heights| NULL|A31JTKBQRVQ83G|\"B. McEwan \"\"yell...|             12/12|         5.0| 1141689600|\"\"\"Why did you be...|           Cathy?\"\"\"|             [cathy]|             [cathy]|                     [cathy]|             [cathy]|\n",
            "|1551114291|   David Copperfield| NULL| A3YW07H3DKVEH|\"Long Ago \"\"Far A...|               4/4|         5.0| 1335744000|    \"\"\"What I reaped|      I had sown.\"\"\"|      [i, had, sown]|              [sown]|                      [sown]|              [sown]|\n",
            "|B000OVMSIW|Scarecrow of Oz (#9)| NULL|A30KEXFT9SILL6|\"frumiousb \"\"frum...|               0/0|         4.0| 1175040000|\"\"\"When the child...| I hope they will...|[i, hope, they, w...|   [hope, let, know]|           [hope, let, know]|   [hope, let, know]|\n",
            "|B0006RWIWK|\"\"\"F\"\" is for fug...| NULL| A4UKBG3M5U5TI|             Westley|               0/0|         4.0| 1338422400|\"\"\"In the time I'...| I'd probably tal...|[i, d, probably, ...|[d, probably, tal...|        [d, probably, tal...|[d, probably, tal...|\n",
            "|5556068054|Alice in Wonderla...| NULL| A4FX5YCJA630V|\"R. M. Fisher \"\"R...|             24/26|         5.0| 1184112000|\"\"\"If You Believe...| I'll Believe in ...|[i, ll, believe, ...|       [ll, believe]|               [ll, believe]|       [ll, believe]|\n",
            "|1847022251|Jane Eyre (Large ...|34.90|A2BPDFR58H9575|              Galina|               2/2|         5.0| 1206921600|\"\"\"I am no bird; ...|          Jane Eyre\"|        [jane, eyre]|        [jane, eyre]|                [jane, eyre]|        [jane, eyre]|\n",
            "|B000N6QKIC|          BEACH ROAD| NULL|A2MF1BP59YMAXW|             Teeitup|               0/0|         1.0| 1169596800|             \"\"\"LAME|                LAME|              [lame]|              [lame]|                      [lame]|              [lame]|\n",
            "|B0008BOPXY|The Wind In the W...| NULL|A2D9IEFJGB483Q|    Kendal B. Hunter|               1/5|         4.0| 1169337600|         \"\"\"Mayberry| RFD\"\" meets \"\"An...|[rfd, meets, anim...|[rfd, meets, anim...|        [rfd, meets, anim...|[rfd, meets, anim...|\n",
            "|0595176976|Abraham, The Drea...|14.95| A5D0IQC9FCAVJ|      Bonnie Carroll|               1/2|         5.0| 1028073600|          \"\"\"Abraham|      The Dreamer\"\"\"|      [the, dreamer]|           [dreamer]|                   [dreamer]|           [dreamer]|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filtering out rows where tokens_clean is empty\n",
        "df_empty_Cleantoken = df_safe.filter((df_safe.tokens_clean.isNotNull()) & (size(df_safe.tokens_clean) > 0))"
      ],
      "metadata": {
        "id": "-TFQQ1X7mXm2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_empty_Cleantoken = df_empty_Cleantoken.dropDuplicates([\"tokens_clean\"])"
      ],
      "metadata": {
        "id": "__NnTIqRmuv8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"features\", numFeatures= 8192)\n",
        "df_featurized_Cleantoken = hashingTF.transform(df_empty_Cleantoken)"
      ],
      "metadata": {
        "id": "aTe_o2VJnAzU"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_featurized_Cleantoken.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf0kcuWhnP29",
        "outputId": "c4e2f573-64eb-48ce-a18f-7a24c439b0b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+------+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "|        Id|               Title| Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|        tokens_clean|            features|\n",
            "+----------+--------------------+------+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "|B000JJVHZE|To Kill A Mocking...|  NULL|          NULL|                NULL|               2/4|         3.0| 1166486400|To Kill A Mocking...|12/19/06To Kill a...|[12, 19, 06to, ki...|[12, 19, 06to, ki...|        [06to, kill, mock...|[06to, kill, mock...|(8192,[109,163,26...|\n",
            "|B0007FI44M|Go tell it on the...|  NULL|          NULL|                NULL|               2/6|         2.0|  920678400|This Book was exc...|I am a 10th grade...|[i, am, a, 10th, ...|[10th, grader, re...|        [10th, grader, re...|[10th, grader, re...|(8192,[198,352,70...|\n",
            "|0446693219|Rich Dad Poor Dad...|  NULL| AXJVJXQQ3FBFK|\"Woodbridge Prope...|               4/5|         5.0| 1188432000|Rich Dad, Poor Da...|My 12yr. old duag...|[my, 12yr, old, d...|[12yr, old, duagh...|        [12yr, old, duagh...|[12yr, old, duagh...|(8192,[1533,2011,...|\n",
            "|B000NXDUEC|The Cat Who Went ...|  NULL|          NULL|                NULL|               4/6|         5.0|  933638400|MORE GREAT MYSTER...|This is the 15th ...|[this, is, the, 1...|[15th, book, seri...|        [15th, book, seri...|[15th, series, ve...|(8192,[282,937,10...|\n",
            "|041536342X|The Egyptian Econ...|200.00|A2BG798H8VV65K|  Herbert J. Statham|               8/8|         5.0| 1137024000|Outstanding study...|\"In the 1980s, as...|[in, the, 1980s, ...|[1980s, post, gra...|        [1980s, post, gra...|[1980s, post, gra...|(8192,[111,174,36...|\n",
            "|052138432X|Computation and H...|142.00| AFOIIAMFWGJTH|                MCMC|               8/8|         4.0| 1123545600|     Dated, but good|\"During the 1980s...|[during, the, 198...|[1980s, two, main...|        [1980s, two, main...|[1980s, two, main...|(8192,[332,401,41...|\n",
            "|1400102138|Beau Geste (Unabr...| 30.39| A77DS5KIS9C2V|         Kara Ortiez|               2/2|         4.0| 1112054400|Three brothers jo...|In the 19th centu...|[in, the, 19th, c...|[19th, century, a...|        [19th, century, a...|[19th, century, a...|(8192,[308,346,37...|\n",
            "|0451519418|Emma (Signet clas...|  NULL| A97D4VLXHVPUE|\"Book worm \"\"Alex\"\"\"|              3/11|         2.0| 1109030400|         I Hate Emma|I do like 19th Ce...|[i, do, like, 19t...|[like, 19th, cent...|        [like, 19th, cent...|[19th, century, f...|(8192,[999,1589,1...|\n",
            "|B0000B0SYF|Beginning Linux P...|  NULL|A20FWKRHFTF903|         Jack Dennon|               3/6|         4.0|  950659200|Should have left ...|The 1st edition w...|[the, 1st, editio...|[1st, edition, we...|        [1st, edition, we...|[1st, edition, do...|(8192,[49,116,191...|\n",
            "|0786228539|     After the Night|  NULL|A2UASR7HS15H7S|\"Janet \"\"cjbookre...|               4/5|         5.0|  947030400|I would give it 6...|This is the 1st L...|[this, is, the, 1...|[1st, lh, book, r...|        [1st, lh, book, r...|[1st, lh, materia...|(8192,[768,1064,1...|\n",
            "+----------+--------------------+------+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "\n",
        "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=10) # You can change the number of hash tables at your will\n",
        "model = mh.fit(df_featurized_Cleantoken)"
      ],
      "metadata": {
        "id": "JAGzOtfd92VV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ctrR3kQg92F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering out rows where tokens_clean is empty\n",
        "df_empty_Cleantoken = df_safe.filter((df_safe.tokens_clean.isNotNull()) & (size(df_safe.tokens_clean) > 0))\n",
        "\n",
        "# Removing duplicates based on clean tokens\n",
        "df_empty_Cleantoken = df_empty_Cleantoken.dropDuplicates([\"tokens_clean\"])\n",
        "\n",
        "# Featurizing the clean tokens\n",
        "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"features\", numFeatures= 8192)\n",
        "df_featurized_Cleantoken = hashingTF.transform(df_empty_Cleantoken)\n",
        "\n",
        "\n",
        "similar_pairs = model.approxSimilarityJoin(\n",
        "    df_featurized_Cleantoken, df_featurized_Cleantoken, threshold=sim_threshold, distCol=\"JaccardDistance\"\n",
        ").filter(\"datasetA.Id < datasetB.Id\")  # to avoid duplicate/reverse pairs\n",
        "\n",
        "similar_pairs = similar_pairs.orderBy(\"JaccardDistance\", ascending=True)"
      ],
      "metadata": {
        "id": "YYtrPCnZ-JIn"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Jaccard Distance is the opposite of Jaccard Similarity. It is defined as 1 - Jaccard Similarity. So, the lower the Jaccard Distance, the more similar the two items are. Hence, we are ordering the pairs by Jaccard Distance in ascending order, in order to get the most similar pairs first."
      ],
      "metadata": {
        "id": "vS45nz5x-X8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similar_pairs.select(\n",
        "    \"datasetA.Title\", \"datasetB.Title\", \"datasetA.Id\", \"datasetB.Id\", \"datasetA.review/text\", \"datasetB.review/text\", \"JaccardDistance\"\n",
        ").show(10, truncate=False)"
      ],
      "metadata": {
        "id": "3M-W5pHH-biT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f18ac803-6708-4fc5-fe62-c3a665669012"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|Title                                                                               |Title                                                                                                              |Id        |Id        |review/text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |review/text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |JaccardDistance     |\n",
            "+------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|The Hobbit                                                                          |Island of Dr. Moreau (Horror Classics)                                                                             |B000GQG7D2|B000KW47M0|\"\"\"The Hobbit                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | or I could not live\"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |0.0                 |\n",
            "|Pride & Prejudice (Classic Library)                                                 |Pride and Prejudice                                                                                                |8188280046|B000GDLGSG|\"\"\"Pride and Prejudice\"\" by Jane Austen                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\"Jane Austen's \"\"Pride and Prejudice\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |0.0                 |\n",
            "|Harper Lee's To Kill a Mockingbird (Barron's Book Notes)                            |To Kill A Mockingbird                                                                                              |0808510258|B000JJVHZE|The fiction novel To Kill A Mockingbird is one of the best well written novels of the 1900s. Harper Lee who is the author of the novel brings out some heartmelting scenes to the reader. She also throws out the sense of what a childhood is like growing up in a country town of Maycomb County. Jem and Scout Finch raised by Atticus Finch who is their father growing up without the presence of their passed mother. Jem and Scout play the investigators of who their neighbor realy is.The Finch's neighbor is the creep talk of the town, but no one realy knows the truth of his life. The scenes of this book are so well described in its southern dialect the reader can picture it realy happening. One great example of this is when Scout walks their neighbor Arthur Radley to his porch and reflects on all of the games, bad times, and good times that happened. Also Lee's courtroom scene is so moving it makes the reader want to go into the book and be part of the trial. Come join Jem and Scout in this mysterious investigation of their mysterious neighbor. Also, go back to the 1930's and get a taste of what the depression was like.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |The fiction novel To Kill A Mockingbird is one of the best well written novels of the 1900s. Harper Lee who is the author of the novel brings out some heartmelting scenes to the reader. She also throws out the sense of what a childhood is like growing up in a country town of Maycomb County. Jem and Scout Finch raised by Atticus Finch who is their father without the presence of their passed mother. Jem and Scout play the investigators of who their neighbor realy is.The Finch's neighbor is the creep talk of the town, but no one realy knows the truth of his life. The scenes of this book are so well described in its southern dialect the reader can picture it realy happening. One great example of this is when Scout walks their neighbor Arthur Radley to his porch and reflects on all of the games, bad times, and good times that happened. Also Lee's courtroom scene is so moving it makes the reader want to go into the book and be part of the trial. Come join Jem and Scout in this mysterious investigation of their mysterious neighbor. Also, go back to the 1930's and get a taste of what the depression was like.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |0.0                 |\n",
            "|The Big Sleep                                                                       |The Castle in the Attic                                                                                            |B000JX09RW|B000NSKB12|\"\"\"Where there is mystery                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |Great Fantasy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |0.0                 |\n",
            "|Augustus: A Novel                                                                   |HERE KITTY KITTY                                                                                                   |0670141127|B000OTUBWY|\"\"\"Augustus\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\"In \"\"Here Kitty                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |0.0                 |\n",
            "|Encyclopedia Brown Takes The Cake (Turtleback School & Library Binding Edition)     |Encyclopedia Brown Tracks Them Down                                                                                |0808549685|B000K0GJUU|\"I remember reading these when I was a kid and was glad to see Encyclopedia back with some new stories.They are great because they make you(and your child) CONCENTRATE on the stories to find the clues to solve the mysteries. My son and I take turns, each reading a page aloud, until we get to the end of the story then we discuss what we think happened and why. Sometimes we are right and sometimes we are wrong. It's alot of fun and it helps him learn to \"\"find the details\"\" in the stories.If you have a grade/middle school child                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\"I remember reading these when I was a kid and was glad to see Encyclopedia back with some new stories.They are great because they make you CONCENTRATE on the stories to find the clues to solve the mysteries. My son and I take turns, each reading a page aloud, until we get to the end of the story then we discuss what we think happened and why. Sometimes we are right and sometimes we are wrong. It's alot of fun and it helps him learn to \"\"find the details\"\" in the stories.If you have a grade/middle school child                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |0.0                 |\n",
            "|The end of the affair                                                               |To kill a mockingbird                                                                                              |B0006WOLMA|B0007DRGI4|That's it. Read the book, not the reviews.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\"\"\"Maycomb                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |0.0                 |\n",
            "|Monstrous Manual (AD&D; 2nd Ed Fantasy Roleplaying Accessory, 2140)                 |The origin of species by means of natural selection, or, The preservation of favored races in the struggle for life|1560766190|B00089KSXW|Very good pictures                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\"\"\"If such do occur                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |0.0                 |\n",
            "|The Chronicles of Narnia: The Lion, The Witch and the Wardrobe (Collector's Edition)|Lion, the Witch, and the Wardrobe                                                                                  |B000G1JYHI|B000KAIZX4|THE LION THE WITCH AND THE WORDROBEI am writing about THE LION THE WITCH AND THE WORDROBE. The story is about 4 kids named Lucy, Peter, Susan and Edmond. They all get sent away because of the war that's happening in there home town. So then they get sent away and they have to stay with a professor. A couple of days later Lucy found a wordrobe in an empty room. When she saw it she went inside and found a....Edmond, and Lucy are trying really hard not to get captured byThe theme of this book is survival. Peter, Susan, the White Witch. In a part of this story Asland is sacrificing himself for Edmond. During the war Peter and his army is fighting the White Witches army.The characters in THE LION THE WITCH AND THE WORDROBE are easy to relate to. The characters are easy to relate to because Edmond doesn't really fit in anywhere. Every time Susan is trying really hard to be smart. Peter just wants stuff to happen and let it be over with. Lucy is the one who cares about anything either it's a person or an animal and she would never lie about anything.The settings of this book are places that would take you everywhere. One of the settings is the White Witches castle. Another one is the professor's house. The Beaver's house and Mr. Tumbnas's house was also a setting in the book. The biggest setting of all is Narnia.Overall I recommend that this book can be read by anyone who loves reading. Everyone should read this book because it's the best book ever. When the author describes the characters it feels like you already know them. The most exciting parts are the settings because the author describes it like you are already there. I also recommend that if you like adventure books then you would really enjoy this book.|THE LION THE WITCH AND THE WORDROBEI am writing about THE LION THE WITCH AND THE WORDROBE. The story is about 4 kids named Lucy, Peter, Susan and Edmond. They all get sent away because of the war that's happening in there home town. So then they get sent away and they have to stay with a professor. A couple of days later Lucy found a wordrobe in an empty room. When she saw it she went inside and found a....The theme of this book is survival. Peter, Susan, Edmond, and Lucy are trying really hard not to get captured by the White Witch. In a part of this story Asland is sacrificing himself for Edmond. During the war Peter and his army is fighting the White Witches army.The characters in THE LION THE WITCH AND THE WORDROBE are easy to relate to. The characters are easy to relate to because Edmond doesn't really fit in anywhere. Every time Susan is trying really hard to be smart. Peter just wants stuff to happen and let it be over with. Lucy is the one who cares about anything either it's a person or an animal and she would never lie about anything.The settings of this book are places that would take you everywhere. One of the settings is the White Witches castle. Another one is the professor's house. The Beaver's house and Mr. Tumbnas's house was also a setting in the book. The biggest setting of all is Narnia.Overall I recommend that this book can be read by anyone who loves reading. Everyone should read this book because it's the best book ever. When the author describes the characters it feels like you already know them. The most exciting parts are the settings because the author describes it like you are already there. I also recommend that if you like adventure books then you would really enjoy this book.|0.011111111111111072|\n",
            "|Relativity: The special and the general theory                                      |Relativity: the Special and General Theory                                                                         |B0007H4TLW|B000H4COKS|In Relativity, Einstein trys to bring his theory of relativity to the masses. When the special and general theorys of relativity were concieved of by Einstein, they revolutionized our perception of space and time. This revolution was so complete that many of the most significant physicists of the time believed that it was nonsense. When Einstein won the Nobel prize for his work on the photoelectric effect, his certificate unequivocally stated that the award was NOT given for his theory of relativity. For much of his life, even Einstein was unwilling to accept some of the predictions of his own work such as black holes.This is all very good, interesting science and history which should be read and understood by everyone. The problem is, though, that Einstein was not a particularly good writer. Einstein is too brilliant for his own good and it shows through frequently in this attempt to stoop to our level. His explanations are usually hard to follow and unintuitive(and I study physics even!). This book exists on an uncomfortable middle ground between rigor and easy reading.If you would like to read this book simply because of its (and its author's) historical significance then I couldn't discourage that. If you know little physics and want to try to understand relativity, read Kip Thorne's Black Holes and Time Warps or the first few chapters of Brian Greene's The Elegant Universe.                                                                                                                                                                                                                                                                                                                                             |In Relativity, Einstein trys to bring his theory of relativity to the masses. When the special and general theorys of relativity were concieved of by Einstein, they revolutionized our perception of space and time. This revolution was so complete that many of the most significant physicists of the time believed that it was nonsense. When Einstein won the Nobel prize for his work on the photoelectric effect, his certificate unequivocally stated that the award was NOT given for his theory of relativity. For much of his life, even Einstein was unwilling to accept some of the predictions of his own work such as black holes.This is all very good, interesting science and history which should be read and understood by everyone. The problem is, though, that Einstein was not a particularly good writer. Einstein is too brilliant for his own good and it shows through frequently in this attempt to stoop to our level. His explanations are usually hard to follow and unintuitive(and I study physics even!). This book exists on an uncomfortable middle ground between rigor and easy reading.If you would like to read this book simply because of its (and its author's) historical significance then I couldn't discourage that. If you know little physics and want to try to understand relativity, however, read Kip Thorne's Black Holes and Time Warps or the first few chapters of Brian Greene's The Elegant Universe.                                                                                                                                                                                                                                                                                                                                     |0.012195121951219523|\n",
            "+------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+----------+----------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}