{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZqcBPCJDgjKQk7/P6ZTbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VIOLDAVE/my_project/blob/main/Massive_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9dY0sWM2uhlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project requirement: https://docs.google.com/document/d/1GkK28wOyjTPZEZuOumKPU0z1NzVT_9sxETjPOcLPVYo/edit?tab=t.0#heading=h.qifoo7co6qtd\n",
        "\n",
        "Professor : https://malchiodi.di.unimi.it/teaching/AMD-DSE/2024-25/en"
      ],
      "metadata": {
        "id": "1jlTqLeCltAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4JVmRCglg7N",
        "outputId": "aedafd50-c6c4-40c4-d3fc-941573248ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'my_project'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 38 (delta 19), reused 18 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (38/38), 25.87 KiB | 1.18 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(\"my_project\"):\n",
        "    !git clone https://github.com/VIOLDAVE/my_project.git\n",
        "\n",
        "if os.path.exists(\"my_project\"):\n",
        "    os.chdir(\"my_project\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Failed to find or clone the 'my_project' directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages."
      ],
      "metadata": {
        "id": "RHFvE1XowXFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "32iMmIFL04_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5467ca-a548-4b79-b9ff-df98a78f13c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 1)) (1.7.4.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 4)) (3.5.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirement.txt (line 6)) (2.2.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirement.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirement.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark->-r requirement.txt (line 4)) (0.10.9.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirement.txt (line 5)) (8.2.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirement.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirement.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirement.txt (line 6)) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " kaggle  set up and Authentication"
      ],
      "metadata": {
        "id": "1ZAJt1DeWM6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "\n",
        "    # Read credentials from the uploaded file\n",
        "    with open(fn, 'r') as f:\n",
        "        creds = json.load(f)\n",
        "        os.environ['KAGGLE_USERNAME'] = 'violaawor2'\n",
        "        os.environ['KAGGLE_KEY'] = 'f93b76093cd1863d38f50e4da7642437'\n",
        "\n",
        "    # Move and rename the uploaded file to the correct kaggle.json path\n",
        "    os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "    os.rename(fn, \"/root/.kaggle/kaggle.json\")\n",
        "    os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Cwgtv5FH0AV-",
        "outputId": "239e58f0-296a-41cf-e7d0-5c59c4cba96e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4c27113-58e9-4169-85d3-4d8b1bd6407d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b4c27113-58e9-4169-85d3-4d8b1bd6407d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle_2.json to kaggle_2 (1).json\n",
            "User uploaded file \"kaggle_2 (1).json\" with length 66 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dowloading files"
      ],
      "metadata": {
        "id": "fWmVG7a4rLko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZQyNBL2qyZ1",
        "outputId": "b1a0a0a8-4bbf-4868-cbf3-e8d823571eb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content/my_project\n",
            "100% 1.06G/1.06G [00:05<00:00, 201MB/s]\n",
            "100% 1.06G/1.06G [00:05<00:00, 192MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unzipping the dataset"
      ],
      "metadata": {
        "id": "GUSv7ijrL074"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"/content/my_project/amazon-books-reviews.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")\n",
        "\n",
        "# List the extracted files to confirm\n",
        "!ls /content/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pV31iADR5Tb",
        "outputId": "12201206-8cdb-4d9e-a8e8-040747a8e6e7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/data\n",
            "books_data.csv\tBooks_rating.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset on Spark"
      ],
      "metadata": {
        "id": "aevDm3w5rY9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset with spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "books_rating = spark.read.csv(\"/content/data/Books_rating.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "hBLdTQKIrZWf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show a few raws of the dataset"
      ],
      "metadata": {
        "id": "GZ5HFPgh3u_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "books_rating.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0eAgajY3yXX",
        "outputId": "aec1327c-8121-409a-9a5f-0f664ca57b65"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|1882931173|Its Only Art If I...| NULL| AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|\"If people become...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A2MVUWT453QH61|\"Roy E. Perry \"\"a...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|\n",
            "|0826414346|Dr. Seuss: Americ...| NULL|A22X4XUPKF66MR|\"D. H. Richards \"...|               3/3|         4.0| 1107993600|Good academic ove...|\"Philip Nel - Dr....|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating a sample of dataset becase the file is too heavy for my colab space"
      ],
      "metadata": {
        "id": "_itQjlr433Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 0.02 # can be adjusted as needed"
      ],
      "metadata": {
        "id": "_G19RGI633rI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "books_sample.show(10)\n",
        "size = books_sample.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_94ae_l48G9",
        "outputId": "ae9105ca-6eae-410d-8e4d-4eee72f0daed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|\n",
            "|0781810698|Beginner's Yoruba...|19.77|A1HHC466MCO9QE|\"ButterflySoulfir...|               7/7|         5.0| 1206057600|Great starting point|\"This book contai...|\n",
            "|0312322291|King James: Belie...| NULL|          NULL|                NULL|               2/2|         5.0| 1118188800|Yo Homie this is ...|\"LeBron James is ...|\n",
            "|B0006D6DRK|Open marriage;: A...| NULL|A3KBF2S2MGN48O|\"T. Thompson \"\"&#...|               2/2|         4.0| 1188604800|A Must Read for a...|This book is a cl...|\n",
            "|0934638160|Grants and Awards...| NULL|          NULL|                NULL|             31/31|         5.0|  950227200|  Worth every penny!|This book offers ...|\n",
            "|0671551345|Night World: Daug...| NULL|          NULL|                NULL|               0/0|         5.0|  889920000|The most charming...|The plot and char...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQhYzI7B5b1F",
        "outputId": "a3d41b4f-8bde-4252-c216-f93a618686bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60498"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking null valuse,. I will use the review column which has strings."
      ],
      "metadata": {
        "id": "e1JQrTWM560x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking  for null values in the 'review/text' column\n",
        "null_df = books_rating.filter(\n",
        "    books_rating['review/text'].isNull() | (books_rating['review/text'].rlike('^\\s*$'))\n",
        ")\n",
        "\n",
        "null_df.show(15)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJLfprIv6U-c",
        "outputId": "9dc3b3ac-2b01-4228-8e6b-78a0154f68fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|        Id|               Title|               Price|             User_id|         profileName|  review/helpfulness|        review/score|         review/time|      review/summary|review/text|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "|B00085T7O2|\"\"\"Catch 'em aliv...|\",,ASEH0CVYVGZCZ,...|                 0/1|                 5.0|          1341705600|Fascinating life ...|There are many id...|                NULL|       NULL|\n",
            "|B00085T7O2|\"\"\"Catch 'em aliv...|\",,A91QQ7IYBAK2Q,...| terrible forward...|I found the book ...|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0007DMFZS|\"\"\"Beloved friend...|\",,AH9F75LQIKEZH,...|                 0/0|                 5.0|          1268611200|Invaluable path t...|\"There is so much...| An American Busi...|       NULL|\n",
            "|B00085P4PI|\"The child Manuel...|\",,A3HN2H8SWQ9V0Z...|                 1/1|                 5.0|          1106438400|Amazing and touching|This book was sim...|                NULL|       NULL|\n",
            "|B0007EFQHG|\"The Nazi \"\"88\"\" ...|\",,,,1/1,5.0,1127...| easy to read and...| Happy P. Abbott....| of any age will ...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0007EFQHG|\"The Nazi \"\"88\"\" ...|\",,A24FY5HONGOBA5...|\"\" it fortified m...|                NULL|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0008CK87U|\"\"\"Gentlemen pref...|\",,,,3/3,5.0,9070...| a directness tha...|       not a cartoon| and therefore ev...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B000GQK706|The Lord of the R...|                NULL|                NULL|                NULL|                 0/1|                 5.0|           938563200|have only one wor...|       NULL|\n",
            "|B00085MA6E|\"Sins of New York...|\",,AT3NJHZMT67KH,...|   Mediocre Stories\"|I must say that I...|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|096719587X|\"Anabolic Outlaw ...|         pot smoking|    cocaine snorting|        pill popping|       acid dropping|    whiskey drinking| steroid shooting...|                4.95|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|\",,A1IZWTVAQVJIFE...| with surprising ...|Looney's double-o...|                NULL|                NULL|                NULL|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|\",,A2SBK4UJEMLDPW...| the main thrust ...| the assertion th...| this will change...| now make sense i...|                NULL|       NULL|\n",
            "|B0006AIAX8|\"\"\"Shakespeare\"\" ...| the seventeenth ...|,,A3Q5N4H6DYX2KP,...|                NULL|                NULL|                NULL|                NULL|                NULL|       NULL|\n",
            "|B000I1VJLA|The Lord of the R...|                NULL|                NULL|                NULL|                 0/1|                 5.0|           938563200|have only one wor...|       NULL|\n",
            "|1594860882|Scrawny to Brawny...|               13.18|      A2PZ208GDOIWZM|\"sloth \"\"\\|/ 3Toe...| and are quoted o...| the best site ou...|                NULL|                NULL|       NULL|\n",
            "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are copies of the'review/text' null entries.\n",
        "\n",
        " Aside from the Title and the IDs, they have the same columns.  A qualitative examination reveals that they are about the same book even though the titles are different."
      ],
      "metadata": {
        "id": "fof3zZqN7PCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are numerous other reviews of Lord of the Rings.  I believe that the NA values should be retained because they cannot be discarded.  The title can also be used as a similarity check."
      ],
      "metadata": {
        "id": "6qeX0MHDAbu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filter_df = books_rating.filter(\n",
        "    col(\"Title\").contains(\"The Lord of the Rings\")\n",
        ").select(\"Title\", \"review/text\").limit(100)\n",
        "\n",
        "# Display result\n",
        "filter_df.show(100, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okOXjXCbAfvQ",
        "outputId": "00848cf4-fc82-4628-ce94-426cc63b1395"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|               Title|         review/text|\n",
            "+--------------------+--------------------+\n",
            "|The Lord of the R...|\"I enjoy these ta...|\n",
            "|The Lord of the R...|This drama comes ...|\n",
            "|The Return of the...|Have the Rings fo...|\n",
            "|The Return of the...|I highly recommen...|\n",
            "|Gollum: How We Ma...|\"Andy Serkis is t...|\n",
            "|Gollum: How We Ma...|I guess I'm a bit...|\n",
            "|The Lord of the R...|I've never attemp...|\n",
            "|The Lord of the R...|This is one of th...|\n",
            "|The Lord of the R...|This made an idea...|\n",
            "|The Lord of the R...|This is one hefty...|\n",
            "|The Lord of the R...|This book is a mu...|\n",
            "|The Lord of the R...|This comprehensiv...|\n",
            "|The Lord of the R...|I just got the bo...|\n",
            "|The Lord of the R...|\"Length:: 2:05 Mi...|\n",
            "|The Lord of the R...|\".\"\"...has left t...|\n",
            "|The Lord of the R...|I will not belabo...|\n",
            "|The Lord of the R...|\"It is amazing th...|\n",
            "|The Lord of the R...|\"Wayne G. Hammond...|\n",
            "|The Lord of the R...|The content of th...|\n",
            "|The Lord of the R...|\"Two of the forem...|\n",
            "|The Lord of the R...|\"As the authors p...|\n",
            "|The Lord of the R...|This extensively ...|\n",
            "|The Lord of the R...|\"This is not a bo...|\n",
            "|The Lord of the R...|For the LoTR affi...|\n",
            "|The Lord of the R...|I read the Lord o...|\n",
            "|The Lord of the R...|\"Length:: 0:24 Mi...|\n",
            "|Tolkien Calendar ...|The 50th Annivers...|\n",
            "|Tolkien Calendar ...|\"This is by far t...|\n",
            "|The Lord of the R...|I have seen other...|\n",
            "|The Lord of the R...|I've seen the LOT...|\n",
            "|The Lord of the R...|Wonderful set bou...|\n",
            "|The Lord of the R...|The Lord of the R...|\n",
            "|The Lord of the R...|And it was fun re...|\n",
            "|The Lord of the R...|Nice to have thre...|\n",
            "|The Lord of the R...|This has long bee...|\n",
            "|The Lord of the R...|So much better th...|\n",
            "|The Lord of the R...|If there were ten...|\n",
            "|The Lord of the R...|Classic Tolkien w...|\n",
            "|The Lord of the R...|One does not simp...|\n",
            "|The Lord of the R...|It might be one o...|\n",
            "|The Lord of the R...|I ordered this fr...|\n",
            "|The Lord of the R...|I read these book...|\n",
            "|The Lord of the R...|I loved it, recom...|\n",
            "|The Lord of the R...|I am reviewing on...|\n",
            "|The Lord of the R...|great book in gre...|\n",
            "|The Lord of the R...|This was a nice b...|\n",
            "|The Lord of the R...|Where to start i ...|\n",
            "|The Lord of the R...|I bought this as ...|\n",
            "|The Lord of the R...|There is little p...|\n",
            "|The Lord of the R...|\"Several reviews ...|\n",
            "|The Lord of the R...|I too have read T...|\n",
            "|The Lord of the R...|The LOTR trilogy ...|\n",
            "|The Lord of the R...|Everyone knows th...|\n",
            "|The Lord of the R...|When this edition...|\n",
            "|The Lord of the R...|Well worth the Am...|\n",
            "|The Lord of the R...|This is an incred...|\n",
            "|The Lord of the R...|This book, with i...|\n",
            "|The Lord of the R...|\"A great book for...|\n",
            "|The Lord of the R...|I agree with the ...|\n",
            "|The Lord of the R...|Whereas other rev...|\n",
            "|The Lord of the R...|\"I've been debati...|\n",
            "|The Lord of the R...|Please listen to ...|\n",
            "|The Lord of the R...|\"I love the Lord ...|\n",
            "|The Lord of the R...|Books can change ...|\n",
            "|The Lord of the R...|Regarding the Lea...|\n",
            "|The Lord of the R...|This Millenium Ed...|\n",
            "|The Lord of the R...|What can I say. T...|\n",
            "|The Lord of the R...|If only I could h...|\n",
            "|The Lord of the R...|Though Tolkien wa...|\n",
            "|The Lord of the R...|Yes, of course th...|\n",
            "|The Lord of the R...|Ok so the world i...|\n",
            "|The Lord of the R...|I remember that I...|\n",
            "|The Lord of the R...|I had bought a Lo...|\n",
            "|The Lord of the R...|While both the bo...|\n",
            "|The Lord of the R...|There's not a tim...|\n",
            "|The Lord of the R...|1) Take a year an...|\n",
            "|The Lord of the R...|I love the LOTR t...|\n",
            "|The Lord of the R...|i have several ve...|\n",
            "|The Lord of the R...|The book that is ...|\n",
            "|The Lord of the R...|Great Price. Grea...|\n",
            "|The Lord of the R...|This was an impor...|\n",
            "|The Lord of the R...|I'm only reviewin...|\n",
            "|The Lord of the R...|\"I'm no expert on...|\n",
            "|The Lord of the R...|This is a very go...|\n",
            "|The Lord of the R...|Great book set! V...|\n",
            "|The Lord of the R...|\"I watch the movi...|\n",
            "|The Lord of the R...|Purchased for son...|\n",
            "|The Lord of the R...|\"This was my firs...|\n",
            "|The Lord of the R...|I purchased this ...|\n",
            "|The Lord of the R...|The set of tapes ...|\n",
            "|The Lord of the R...|This is a wonderf...|\n",
            "|The Lord of the R...|i have listened t...|\n",
            "|The Lord of the R...|\"I love the Lord ...|\n",
            "|The Lord of the R...|This book is magn...|\n",
            "|The Lord of the R...|Ok, so I had been...|\n",
            "|The Lord of the R...|I have been a Tol...|\n",
            "|The Lord of the R...|These books are s...|\n",
            "|The Lord of the R...|\"\"\"The Lord of th...|\n",
            "|The Lord of the R...|I've read the LOT...|\n",
            "|The Lord of the R...|This boxed set ha...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null values in subsample books."
      ],
      "metadata": {
        "id": "VQqnaZkRBJIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the 'review/text' column\n",
        "Subsample_book = books_sample.filter(\n",
        "    books_sample['review/text'].isNull() | (books_sample['review/text'].rlike('^\\s*$'))\n",
        ")\n",
        "#Display sample books\n",
        "Subsample_book.show(35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW6xSRe2Beju",
        "outputId": "7ba7e9ee-23fd-415b-9bdd-2a50f711bd63"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+-------+-----------+------------------+------------+-----------+--------------------+-----------+\n",
            "|        Id|               Title|Price|User_id|profileName|review/helpfulness|review/score|review/time|      review/summary|review/text|\n",
            "+----------+--------------------+-----+-------+-----------+------------------+------------+-----------+--------------------+-----------+\n",
            "|B000I1VJLA|The Lord of the R...| NULL|   NULL|       NULL|               0/1|         5.0|  938563200|have only one wor...|       NULL|\n",
            "+----------+--------------------+-----+-------+-----------+------------------+------------+-----------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jaccard Similarity\n",
        "\n",
        "\n",
        "#Tokenisation\n",
        " To test the Jaccard Similarity technique, I will utilise a subsample of the dataset because it is too large.  After that, I'll attempt to top scale it to the entire dataset.  Additionally, I'll parallelise the Jaccard Similarity calculation using the MapReduce technique."
      ],
      "metadata": {
        "id": "18-3_LwWYpla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType"
      ],
      "metadata": {
        "id": "Q8uM7-rxY4OO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\") # \\W is a regex pattern that matches any non-word character\n"
      ],
      "metadata": {
        "id": "OR2WoGz-Z-Pz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QL2OgbHea40Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4ea97f3"
      },
      "source": [
        "## Apply tokenization\n",
        "\n",
        "### Subtask:\n",
        "Apply the `RegexTokenizer` to the `review/text` column of your Spark DataFrame to get tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ec07469"
      },
      "source": [
        "**Meaning**\n",
        "Apply the pre-defined RegexTokenizer to the books_sample DataFrame and display the schema of the resulting DataFrame to confirm the new column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qDV9F9W9e4ls"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d12a478",
        "outputId": "3690d9b7-ae2d-4def-e3f1-696791aac3db"
      },
      "source": [
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be975d7"
      },
      "source": [
        "\n",
        "The error indicates that the `tokenizer` object was not defined. This is because the previous cell that defined it was not executed in the current runtime. I need to redefine the tokenizer before using it to transform the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxIwTDSUYB1E",
        "outputId": "e2eac751-16b3-4673-bad3-f35e531ff449"
      },
      "source": [
        "from pyspark.ml.feature import RegexTokenizer\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2bed5e"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `books_sample` is not defined. This is because the cell where `books_sample` was created by sampling `books_rating` was not executed in the current runtime. I need to recreate the `books_sample` DataFrame before applying the tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6sTxO2HYEKh",
        "outputId": "932b6ec4-f396-4573-cdbf-e61605f45261"
      },
      "source": [
        "sample = 0.02\n",
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe945fe"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `books_rating` is not defined. This is because the cell where `books_rating` was loaded was not executed in the current runtime. I need to load the `books_rating` DataFrame before creating the sample and applying the tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTqAEJFGYFoJ",
        "outputId": "a6a491c6-92ba-4954-9423-ffc0d4da4e77"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "books_rating = spark.read.csv(\"/content/data/Books_rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "sample = 0.02\n",
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16ca20f3"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the file path `/content/data/Books_rating.csv` does not exist. This is likely because the previous unzipping step was not executed in the current runtime, or the path is incorrect. I need to ensure the file is unzipped and then load the data. The previous unzipping code cell used `/content/data` as the extraction path, so the path in the loading code should be correct if the unzipping is done. I will include the unzipping step before loading the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFZ3O78jYHpz",
        "outputId": "1112ccf2-80bd-428b-f096-b409663c8f77"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "\n",
        "zip_file_path = \"/content/my_project/amazon-books-reviews.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "books_rating = spark.read.csv(\"/content/data/Books_rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "sample = 0.02\n",
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/data\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48356303"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the zip file `/content/amazon-books-reviews.zip` was not found. This means the Kaggle download step was not executed in the current runtime. I need to download the dataset using the Kaggle API before attempting to unzip and load it again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvuW4y8vYJj7",
        "outputId": "bd6d569b-e079-4056-bf4c-6c6cdbcb39d8"
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "import kaggle\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews -p /content/ --unzip\n",
        "\n",
        "zip_file_path = \"/content/amazon-books-reviews.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file if it wasn't unzipped by kaggle command\n",
        "if not os.path.exists(os.path.join(extract_path, 'Books_rating.csv')):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to: {extract_path}\")\n",
        "\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "books_rating = spark.read.csv(os.path.join(extract_path, \"Books_rating.csv\"), header=True, inferSchema=True)\n",
        "\n",
        "sample = 0.02\n",
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 97% 1.03G/1.06G [00:03<00:00, 265MB/s]\n",
            "100% 1.06G/1.06G [00:03<00:00, 292MB/s]\n",
            "Files extracted to: /content/data\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed5d66b7"
      },
      "source": [
        "\n",
        "The error indicates that the zip file was not found even after the Kaggle download command. However, the Kaggle command includes the `--unzip` flag, which should extract the files directly to the specified path. Therefore, the manual unzipping step is likely redundant and causing the error because the zip file is no longer in the expected location after the Kaggle command extracts its contents. I should remove the manual unzipping and directly load the CSV from the directory specified in the Kaggle download command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUE85SKYYZvX",
        "outputId": "85efacb3-bbf6-4856-9cf5-011128de6fee"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer\n",
        "import os\n",
        "import kaggle\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews -p /content/data --unzip\n",
        "\n",
        "spark = SparkSession.builder.appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "books_rating = spark.read.csv(\"/content/data/Books_rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "sample = 0.02\n",
        "books_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\")\n",
        "tokenized_df = tokenizer.transform(books_sample)\n",
        "tokenized_df.printSchema()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content/data\n",
            " 98% 1.04G/1.06G [00:16<00:01, 19.7MB/s]\n",
            "100% 1.06G/1.06G [00:16<00:00, 68.0MB/s]\n",
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8894ba39"
      },
      "source": [
        "## Remove stop words\n",
        "\n",
        "### Subtask:\n",
        "Use `StopWordsRemover` to remove common English stop words from the tokenized reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "889f81da"
      },
      "source": [
        "**Reasoning**:\n",
        "Use StopWordsRemover to remove common English stop words from the tokenized reviews.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eaf9030",
        "outputId": "70dbcaff-2949-4506-9ed7-9f4bd8d8011c"
      },
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"reviews/filtered_tokens\")\n",
        "filtered_df = remover.transform(tokenized_df)\n",
        "filtered_df.printSchema()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            " |-- reviews/tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- reviews/filtered_tokens: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f0e3b8"
      },
      "source": [
        "## Calculate jaccard similarity\n",
        "\n",
        "### Subtask:\n",
        "Implement a method to calculate the Jaccard similarity between the processed review tokens for pairs of books.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc5dcc80"
      },
      "source": [
        "\n",
        " Python function to calculate Jaccard similarity and register it as a Spark UDF. Then, join the DataFrame with itself to create pairs of books and apply the UDF to calculate the similarity between their filtered tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55f5af91",
        "outputId": "ce128cc7-60d5-4c06-cbac-c85ca8b4ebe7"
      },
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    if not list1 or not list2:\n",
        "        return 0.0\n",
        "    set1 = set(list1)\n",
        "    set2 = set(list2)\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0.0\n",
        "\n",
        "jaccard_udf = udf(jaccard_similarity, DoubleType())\n",
        "\n",
        "# Create pairs of books for similarity calculation\n",
        "book_pairs = filtered_df.alias(\"df1\").join(\n",
        "    filtered_df.alias(\"df2\"),\n",
        "    col(\"df1.Id\") < col(\"df2.Id\")\n",
        ")\n",
        "\n",
        "# Calculate Jaccard similarity for each pair\n",
        "similarity_df = book_pairs.withColumn(\n",
        "    \"jaccard_similarity\",\n",
        "    jaccard_udf(col(\"df1.reviews/filtered_tokens\"), col(\"df2.reviews/filtered_tokens\"))\n",
        ")\n",
        "\n",
        "# Select relevant columns and show the results\n",
        "similarity_df.select(\n",
        "    col(\"df1.Id\").alias(\"book1_id\"),\n",
        "    col(\"df2.Id\").alias(\"book2_id\"),\n",
        "    col(\"jaccard_similarity\")\n",
        ").show(20)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+--------------------+\n",
            "|  book1_id|  book2_id|  jaccard_similarity|\n",
            "+----------+----------+--------------------+\n",
            "|0595344550|B000NKGYMK|0.056338028169014086|\n",
            "|0595344550|B000NKGYMK|0.013513513513513514|\n",
            "|0595344550|157067051X|0.015151515151515152|\n",
            "|0595344550|B0007DVHU2|0.009523809523809525|\n",
            "|0595344550|0781810698|0.013888888888888888|\n",
            "|0595344550|B0006D6DRK|  0.0425531914893617|\n",
            "|0595344550|0934638160|0.013333333333333334|\n",
            "|0595344550|0671551345|0.029850746268656716|\n",
            "|0595344550|0671551345|0.011363636363636364|\n",
            "|0595344550|1555714137| 0.05426356589147287|\n",
            "|0595344550|B0000630MU|0.012195121951219513|\n",
            "|0595344550|B0008852GG|0.017543859649122806|\n",
            "|0595344550|B000MCKQRS|0.047619047619047616|\n",
            "|0595344550|155558182X|  0.0380952380952381|\n",
            "|0595344550|0882899228|0.020202020202020204|\n",
            "|0595344550|B000O3QCH8| 0.05263157894736842|\n",
            "|0595344550|B000O3QCH8|0.038834951456310676|\n",
            "|0595344550|0836204271|0.023809523809523808|\n",
            "|0595344550|0932813062|0.031914893617021274|\n",
            "|0595344550|B0006AUI9W|                 0.0|\n",
            "+----------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nKGtYB6PirsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "#from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType"
      ],
      "metadata": {
        "id": "UHZbsToRjhyS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\W\") # \\W is a regex pattern that matches any non-word character"
      ],
      "metadata": {
        "id": "-GMXb3xqnf0Y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text is tokenised into words.  Since this method uses sparks, I think it will be quicker than certain neural network-based methods, therefore I'm giving it a shot.  Although it will have certain drawbacks because it ignores compound terms, I believe the first strategy will benefit from it."
      ],
      "metadata": {
        "id": "_QQAE_ucn1Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"tokens/nostopwords\")"
      ],
      "metadata": {
        "id": "FxqAfBW7nvDc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A predetermined list of English stop words is used by StopWordsRemover().  Common terms like \"the,\" \"is,\" \"in,\" and others that don't have much meaning are eliminated.  This is significant because the number of words that two texts share determines the Jaccard Similarity.  The quantity of stop words in the texts, which are irrelevant to their meaning, will skew the Jaccard Similarity if they are not eliminated."
      ],
      "metadata": {
        "id": "rrxTLBGTobpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "    return [token for token in tokens if not token.isdigit()]"
      ],
      "metadata": {
        "id": "87T7-u0wo-61"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequently, the meaning of a review is not affected by numbers.  We can eliminate the numbers from the text because the score in our dataset is kept in a distinct column."
      ],
      "metadata": {
        "id": "P8y7jQDcpEly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "remove_numbers_udf = udf(remove_numbers, ArrayType(StringType())) # Specify the return type as ArrayType(StringType), meaning an array/list of strings\n"
      ],
      "metadata": {
        "id": "ANn2O-vCpck_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because Sparks is not a Python object, it must be used in the pipeline as a user-defined function (UDF)."
      ],
      "metadata": {
        "id": "IJZz_8VcpfZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_books_sample = tokenizer.transform(books_sample) # Apply the tokenizer to the dataset\n",
        "tokenized_reviews_nostopwords_sample = stopwords_remover.transform(tokenized_books_sample) # Remove the stop words from the tokenized dataset\n",
        "tokenized_rw_nsw_nn_sample = tokenized_reviews_nostopwords_sample.withColumn(\"tokens/nostopwords/nonumbers\", remove_numbers_udf(col(\"tokens/nostopwords\"))) # Remove the numbers from the dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "vLzVFbMapo8H"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uvkyH5H6tVHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "books_sample.show(10)\n",
        "tokenized_rw_nsw_nn_sample.show(10, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLD-3q9tZvO",
        "outputId": "5029787f-4d15-4584-b8ec-74d38b968a9a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|\n",
            "|0781810698|Beginner's Yoruba...|19.77|A1HHC466MCO9QE|\"ButterflySoulfir...|               7/7|         5.0| 1206057600|Great starting point|\"This book contai...|\n",
            "|0312322291|King James: Belie...| NULL|          NULL|                NULL|               2/2|         5.0| 1118188800|Yo Homie this is ...|\"LeBron James is ...|\n",
            "|B0006D6DRK|Open marriage;: A...| NULL|A3KBF2S2MGN48O|\"T. Thompson \"\"&#...|               2/2|         4.0| 1188604800|A Must Read for a...|This book is a cl...|\n",
            "|0934638160|Grants and Awards...| NULL|          NULL|                NULL|             31/31|         5.0|  950227200|  Worth every penny!|This book offers ...|\n",
            "|0671551345|Night World: Daug...| NULL|          NULL|                NULL|               0/0|         5.0|  889920000|The most charming...|The plot and char...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|[this, is, a, sel...|[self, published,...|        [self, published,...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A258YNWJW2264M|\"Tessa F. Briggs ...|              8/14|         3.0| 1241827200|Not your quick re...|\"After having a c...|[after, having, a...|[chance, read, bo...|        [chance, read, bo...|\n",
            "|B000NKGYMK|    Alaska Sourdough| NULL|A2WY5VMJQ0MM1A|                 Aoy|               0/0|         5.0| 1351641600|        Old and good|This book is wort...|[this, book, is, ...|[book, worth, kee...|        [book, worth, kee...|\n",
            "|157067051X|Voices from the F...| NULL|A1WBUHQIIHUA8O|          L. Maxwell|               1/1|         4.0| 1178496000|Voices from the F...|Gave me a detaile...|[gave, me, a, det...|[gave, detailed, ...|        [gave, detailed, ...|\n",
            "|B0007DVHU2|Treat yourself to...| NULL|A1RJD10TTI568L|\"Pieter Uys \"\"Toy...|               4/4|         5.0| 1239321600|Harnessing though...|\"Dr Baker was one...|[dr, baker, was, ...|[dr, baker, one, ...|        [dr, baker, one, ...|\n",
            "|0781810698|Beginner's Yoruba...|19.77|A1HHC466MCO9QE|\"ButterflySoulfir...|               7/7|         5.0| 1206057600|Great starting point|\"This book contai...|[this, book, cont...|[book, contains, ...|        [book, contains, ...|\n",
            "|0312322291|King James: Belie...| NULL|          NULL|                NULL|               2/2|         5.0| 1118188800|Yo Homie this is ...|\"LeBron James is ...|[lebron, james, i...|[lebron, james, b...|        [lebron, james, b...|\n",
            "|B0006D6DRK|Open marriage;: A...| NULL|A3KBF2S2MGN48O|\"T. Thompson \"\"&#...|               2/2|         4.0| 1188604800|A Must Read for a...|This book is a cl...|[this, book, is, ...|[book, classic, p...|        [book, classic, p...|\n",
            "|0934638160|Grants and Awards...| NULL|          NULL|                NULL|             31/31|         5.0|  950227200|  Worth every penny!|This book offers ...|[this, book, offe...|[book, offers, cl...|        [book, offers, cl...|\n",
            "|0671551345|Night World: Daug...| NULL|          NULL|                NULL|               0/0|         5.0|  889920000|The most charming...|The plot and char...|[the, plot, and, ...|[plot, characters...|        [plot, characters...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a vocabulary of the tokens, we require CountVectorizer().  The most common tokens in the dataset will be compiled into a vocabulary.  The vocabulary's maximum size is determined by the vocabularySize.  The minimum quantity of documents containing a token in order for it to be incorporated into the vocabulary is known as the minDF parameter.  This is crucial since we wish to eliminate uncommon tokens that don't add to the texts' meaning."
      ],
      "metadata": {
        "id": "ll3Bd3_ruDup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "df_safe = tokenized_rw_nsw_nn_sample \\\n",
        "    .filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens/nostopwords/nonumbers\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model = cv.fit(df_safe)\n",
        "vocab = model.vocabulary\n"
      ],
      "metadata": {
        "id": "yIBBW7DItmmf"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words = model.vocabulary[:30]\n",
        "print(\"Top 30 tokens (approx. by frequency):\")\n",
        "for i, token in enumerate(top_words, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KhrRDwmuyWx",
        "outputId": "c389a911-201d-4889-c15d-f72b5beb5996"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 tokens (approx. by frequency):\n",
            " 1. book\n",
            " 2. read\n",
            " 3. one\n",
            " 4. quot\n",
            " 5. story\n",
            " 6. like\n",
            " 7. books\n",
            " 8. time\n",
            " 9. good\n",
            "10. great\n",
            "11. reading\n",
            "12. life\n",
            "13. first\n",
            "14. well\n",
            "15. many\n",
            "16. much\n",
            "17. people\n",
            "18. love\n",
            "19. really\n",
            "20. also\n",
            "21. characters\n",
            "22. novel\n",
            "23. even\n",
            "24. get\n",
            "25. way\n",
            "26. author\n",
            "27. world\n",
            "28. think\n",
            "29. written\n",
            "30. years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top 30 terms in the vocaboulary are roughly represented by this way.  However, since CountVectorizer() merely approximates the actual frequency of the top words, it does not display the frequency, I'm looking for"
      ],
      "metadata": {
        "id": "9p4d5TqUwhdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "exploded = df_safe.select(explode(col(\"tokens/nostopwords/nonumbers\")).alias(\"token\"))\n",
        "\n",
        "# Filtering only the top-20 tokens\n",
        "topK_set = set(model.vocabulary[:20])  # top list  20 from vocabulary\n",
        "filtered = exploded.filter(col(\"token\").isin(topK_set))\n",
        "\n",
        "exact_counts = (filtered.groupBy(\"token\").count().orderBy(col(\"count\").desc()))\n",
        "exact_counts.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNBPHJfywlA7",
        "outputId": "eaee5a23-a1eb-4ca5-b67e-e25da298640c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|token  |count|\n",
            "+-------+-----+\n",
            "|book   |96856|\n",
            "|read   |40403|\n",
            "|one    |29119|\n",
            "|quot   |26373|\n",
            "|story  |20064|\n",
            "|like   |18044|\n",
            "|books  |14916|\n",
            "|time   |14630|\n",
            "|good   |14489|\n",
            "|great  |14359|\n",
            "|reading|13641|\n",
            "|life   |13370|\n",
            "|first  |12788|\n",
            "|well   |12378|\n",
            "|many   |12281|\n",
            "|much   |11090|\n",
            "|people |10652|\n",
            "|love   |10610|\n",
            "|really |10566|\n",
            "|also   |10528|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8WyTscACx7fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.30 * size # % of the sample size and can be adjust\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "OMrH5FLX1m_O",
        "outputId": "ce0ca2df-d923-4923-e94f-866945fddab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18149.399999999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highest_freq_tokens = (exact_counts.filter(col(\"count\") > threshold).select(\"token\")\n",
        "      .rdd.flatMap(lambda x: x)\n",
        "      .collect()\n",
        ")\n"
      ],
      "metadata": {
        "id": "VS1YHVN819WT"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_freq_tokens"
      ],
      "metadata": {
        "id": "8y35tguG2rFZ",
        "outputId": "39f798ed-47a6-4172-e9a2-ca8a40cfc4f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book', 'read', 'one', 'quot', 'story']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scalability"
      ],
      "metadata": {
        "id": "XLHhr5bo3Msl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must filter out the entries that contain empty lists because Sparks does not function properly with them.  To accomplish this, we may use the size() and isNull() functions to determine whether the list is null or empty.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "alck-5iA-ZOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering out stop words in the tokenised data."
      ],
      "metadata": {
        "id": "dz1vRb4PiLel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_custom_stopwords(tokens):\n",
        "    return [t for t in tokens if t not in highest_freq_tokens]\n",
        "\n",
        "remove_udf = udf(remove_custom_stopwords, ArrayType(StringType()))\n",
        "\n",
        "tokenized_sample_2 = tokenized_rw_nsw_nn_sample.withColumn(\n",
        "    \"tokens_clean\",\n",
        "    remove_udf(col(\"tokens/nostopwords/nonumbers\"))\n",
        ")"
      ],
      "metadata": {
        "id": "41bSZZPqg49F"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "df_safe_clean = tokenized_sample_2.filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "empty_tokens_count = df_safe_clean.filter(\n",
        "    (size(col(\"tokens_clean\")) == 0) |\n",
        "    (col(\"tokens_clean\").isNull())\n",
        ").count()\n",
        "\n",
        "print(f\"clean empty tokens: {empty_tokens_count}\")"
      ],
      "metadata": {
        "id": "oAIdHqBViEuu",
        "outputId": "a18639de-b0cc-4cf4-e5c6-10941e63bffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean empty tokens: 106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering rows with empty list becasue spark doesnot work well with lists."
      ],
      "metadata": {
        "id": "3V20rhSNjKOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens_clean\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model2 = cv.fit(df_safe_clean)\n",
        "vocab2 = model2.vocabulary\n"
      ],
      "metadata": {
        "id": "Vt5S1H9zBMaB"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "different top words"
      ],
      "metadata": {
        "id": "qdw_iioOCtUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top20 = model2.vocabulary[:20]\n",
        "print(top20)\n",
        "for i, token in enumerate(top20, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "id": "g5p35TUmBvP9",
        "outputId": "d4816422-7685-4e2d-d28d-68d827814f00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['like', 'books', 'time', 'good', 'great', 'reading', 'life', 'first', 'well', 'many', 'much', 'people', 'love', 'really', 'also', 'characters', 'novel', 'even', 'get', 'way']\n",
            " 1. like\n",
            " 2. books\n",
            " 3. time\n",
            " 4. good\n",
            " 5. great\n",
            " 6. reading\n",
            " 7. life\n",
            " 8. first\n",
            " 9. well\n",
            "10. many\n",
            "11. much\n",
            "12. people\n",
            "13. love\n",
            "14. really\n",
            "15. also\n",
            "16. characters\n",
            "17. novel\n",
            "18. even\n",
            "19. get\n",
            "20. way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Duplicates\n",
        "df_safe = df_safe_clean.dropDuplicates([\"review/text\"])"
      ],
      "metadata": {
        "id": "sFsRNb0gB8XF"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Local Sensitive Hashing\n",
        "is a method that hashes similar input items into the same \"buckets\" with high probability. It is used to find similar items in large datasets efficiently."
      ],
      "metadata": {
        "id": "af7c_igIC8Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_threshold = 0.6 # You can change it at your will\n"
      ],
      "metadata": {
        "id": "IJN0ZB1XDnI6"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_safe.show(10)"
      ],
      "metadata": {
        "id": "INPdEKqgD1ev",
        "outputId": "2af1e95c-bd73-407d-f4a8-2b5d98d1989b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|        tokens_clean|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "|0595229751|Tranceformers: Sh...| NULL|A3N4LVV9SE7BM8|\"Shea Bower \"\"Dr....|             15/20|         5.0| 1159401600|           \"\"\"Lights|        ..... Action|            [action]|            [action]|                    [action]|            [action]|\n",
            "|B000EANQJ8|Manhattan Stories...| NULL|A1E8H1RAP28FYK|              Lauren|               1/1|         5.0| 1095638400|\"\"\"The Great Gats...|        Extravagence|      [extravagence]|      [extravagence]|              [extravagence]|      [extravagence]|\n",
            "|B000FJN896|The Decline and F...| NULL|A1DW4U1LQV0XW7|  Kelsey May Dangelo|               2/2|         4.0| 1189641600|\"\"\"It's easy to s...|              I know|           [i, know]|              [know]|                      [know]|              [know]|\n",
            "|5556068054|Alice in Wonderla...| NULL| A4FX5YCJA630V|\"R. M. Fisher \"\"R...|             24/26|         5.0| 1184112000|\"\"\"If You Believe...| I'll Believe in ...|[i, ll, believe, ...|       [ll, believe]|               [ll, believe]|       [ll, believe]|\n",
            "|1847022251|Jane Eyre (Large ...|34.90|A2BPDFR58H9575|              Galina|               2/2|         5.0| 1206921600|\"\"\"I am no bird; ...|          Jane Eyre\"|        [jane, eyre]|        [jane, eyre]|                [jane, eyre]|        [jane, eyre]|\n",
            "|1414307586|          Revolution|13.13|A2C51SOFLJH95B| truck driver writer|               0/0|         5.0| 1318636800|\"To understand Ba...|         a novel\"\".\"|          [a, novel]|             [novel]|                     [novel]|             [novel]|\n",
            "|0394755359|  Ultramarine: Poems|14.95| ACSC6M1COW4JE|        W. B. Abbott|               0/0|         5.0| 1361750400|        \"\"\"The Car\"\"|               alone|             [alone]|             [alone]|                     [alone]|             [alone]|\n",
            "|4770029942|Old Kyoto: The Up...| NULL|A2B8GXSCB1R05T|       Zack Davisson|               5/5|         5.0| 1111968000|\"\"\"Down the cobbl...| and behind the t...|[and, behind, the...|[behind, tranquil...|        [behind, tranquil...|[behind, tranquil...|\n",
            "|1847026672|The Small House a...|42.90|A319KYEIAZ3SON|        Mary Whipple|               4/4|         5.0| 1225152000|\"\"\"The claims of ...| but those of lov...|[but, those, of, ...|   [love, paramount]|           [love, paramount]|   [love, paramount]|\n",
            "|0192833669|Frankenstein or T...| NULL|A20EEWWSFMZ1PN|  \"bernie \"\"xyzzy\"\"\"|               1/3|         3.0| 1096761600|           \"\"\"Cursed|  cursed creator.\"\"\"|   [cursed, creator]|   [cursed, creator]|           [cursed, creator]|   [cursed, creator]|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter out rows where tokens_clean is empty\n",
        "df_empty_Cleantoken = df_safe.filter((df_safe.tokens_clean.isNotNull()) & (size(df_safe.tokens_clean) > 0))"
      ],
      "metadata": {
        "id": "-TFQQ1X7mXm2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_empty_Cleantoken = df_empty_Cleantoken.dropDuplicates([\"tokens_clean\"])"
      ],
      "metadata": {
        "id": "__NnTIqRmuv8"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF\n",
        "hashingTF = HashingTF(inputCol=\"tokens_clean\", outputCol=\"features\", numFeatures= 8192)\n",
        "df_featurized_Cleantoken = hashingTF.transform(df_empty_Cleantoken)"
      ],
      "metadata": {
        "id": "aTe_o2VJnAzU"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_featurized_Cleantoken.show(10)"
      ],
      "metadata": {
        "id": "vf0kcuWhnP29",
        "outputId": "aa95ec15-1917-4e76-ccef-8b724dc7776a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|      reviews/tokens|  tokens/nostopwords|tokens/nostopwords/nonumbers|        tokens_clean|            features|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "|B0007HGBGI|       Black like me| NULL|          NULL|                NULL|              5/10|         5.0| 1031443200|Black Like Me Boo...|... ...09/07/02Co...|[09, 07, 02core, ...|[09, 07, 02core, ...|        [02core, 8book, r...|[02core, 8book, r...|(8192,[21,236,316...|\n",
            "|0334040957|WRESTLING WITH AN...| NULL|A14OJS0VWMOSWO| Midwest Book Review|               6/6|         5.0| 1209772800|A deeper look int...|\"The 104th Archbi...|[the, 104th, arch...|[104th, archbisho...|        [104th, archbisho...|[104th, archbisho...|(8192,[551,921,13...|\n",
            "|076240552X|The Scarlet Lette...| NULL|A27AJS71W0R345|               Kayce|               0/0|         3.0| 1356998400|        Interesting.|I had to read thi...|[i, had, to, read...|[read, 10th, grad...|        [read, 10th, grad...|[10th, grade, eng...|(8192,[1575,2120,...|\n",
            "|B000JJVHZE|To Kill A Mocking...| NULL| A5HJHFBPOTYEZ|\"Stephanie An Aza...|               0/0|         5.0| 1049241600|        Outstanding!|I read this book ...|[i, read, this, b...|[read, book, 10th...|        [read, book, 10th...|[10th, grade, fav...|(8192,[2692,3499,...|\n",
            "|B000L5XWTA|     Of Mice and Men| NULL|          NULL|                NULL|               0/0|         4.0|  979084800|AP English review...|I had to read thi...|[i, had, to, read...|[read, book, 10th...|        [read, book, 10th...|[10th, grade, wel...|(8192,[377,596,79...|\n",
            "|B000JQXNSQ|ANIMAL FARM - A F...| NULL|          NULL|                NULL|               0/0|         5.0|  906508800|        THE Book!!!!|As a 10th grader,...|[as, a, 10th, gra...|[10th, grader, ma...|        [10th, grader, ma...|[10th, grader, ma...|(8192,[39,768,799...|\n",
            "|B000GWOWC4|Welcome to the Mo...| NULL|A1EZB5VRCAEQZ5|  \"Barb \"\"private\"\"\"|               2/2|         5.0| 1096329600|Way back in Highs...|I read this book ...|[i, read, this, b...|[read, book, 11th...|        [read, book, 11th...|[11th, grade, int...|(8192,[346,422,11...|\n",
            "|B000IZ76B8|  Gone with the Wind| NULL|A2Y4FX8NDPA2NI|            Brittany|               3/5|         5.0|  943833600|This Author is Good!|Me being 11yrs. o...|[me, being, 11yrs...|[11yrs, old, didn...|        [11yrs, old, didn...|[11yrs, old, didn...|(8192,[33,109,788...|\n",
            "|0439339049|My Thirteenth Win...| NULL|          NULL|                NULL|               6/7|         3.0| 1143590400|              Taylor|In My 13th Winter...|[in, my, 13th, wi...|[13th, winter, se...|        [13th, winter, se...|[13th, winter, se...|(8192,[144,247,36...|\n",
            "|B000BI4D9A|Chicka Chicka Boo...| NULL|A1FAHD4J10EVQM|                Nova|               0/0|         5.0| 1295740800|         Good books!|My 14th month son...|[my, 14th, month,...|[14th, month, son...|        [14th, month, son...|[14th, month, son...|(8192,[737,863,14...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+--------------------+--------------------+----------------------------+--------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}